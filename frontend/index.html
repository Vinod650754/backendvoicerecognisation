<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Voice Assistant Recorder & Live Tester</title>
  <style>
    body { font-family: Arial, sans-serif; max-width: 900px; margin: 20px auto; }
    h1 { margin-bottom: 6px }
    .panel { border: 1px solid #ddd; padding: 12px; margin-bottom: 14px }
    button { margin-right: 8px }
    canvas { width: 100%; height: 80px; background: #111; display:block }
    #recognized { white-space: pre-wrap; background:#f6f6f6; padding:8px; border-radius:4px }
  </style>
</head>
<body>
  <h1>Voice Assistant â€” Record & Live Test</h1>

  <div class="panel">
    <h3>1) Record Owner Voice Samples (for biometric training)</h3>
    <p style="font-size:0.9em; color:#666; margin-top:-8px; margin-bottom:8px;">
      Record your voice for wakeword and commands. These will be saved as <strong>owner voice</strong> for biometric training.
    </p>
    <label>Sample type:
      <select id="sampleType"><option value="wakewords">Wakeword</option><option value="commands">Command</option></select>
    </label>
    <label>Label:
      <select id="sampleLabel">
        <option value="wakeword">wakeword</option>
        <option value="noise">noise</option>
        <option value="open_door">open_door</option>
        <option value="close_door">close_door</option>
      </select>
    </label>
    <div style="margin-top:8px">
      <button id="recStart">Start Recording</button>
      <button id="recStop">Stop</button>
      <button id="uploadSample">Upload (saves as owner voice)</button>
      <span id="recordStatus"></span>
    </div>
  </div>

  <div class="panel">
    <h3>2) Live assistant (always listening â€” say your wakeword or command)</h3>
    <div><canvas id="wave"></canvas></div>
    <h4>Status</h4>
    <div id="assistantState" style="font-weight:bold;color:#0a0">(initializing...)</div>
    <button id="testAudioBtn" style="margin-top:10px">ðŸ§ª Test Audio Upload (Debug)</button>
    <span id="verifiedLight" title="Biometric verified" style="display:inline-block;margin-left:12px;vertical-align:middle;width:14px;height:14px;border-radius:50%;background:#777;border:1px solid #444"></span>
    <h4>Recognized command</h4>
    <div id="recognized">(listening...)</div>
  </div>

  <script>
    // --- Sample collection recording/upload ---
    let coll_mediaStream = null
    let coll_mediaRecorder = null
    let coll_recordedBlobs = null
    const recStart = document.getElementById('recStart')
    const recStop = document.getElementById('recStop')
    const uploadBtn = document.getElementById('uploadSample')
    const testAudioBtn = document.getElementById('testAudioBtn')
    const recordStatus = document.getElementById('recordStatus')

    recStart.addEventListener('click', async () => {
      coll_mediaStream = await navigator.mediaDevices.getUserMedia({audio:true})
      coll_recordedBlobs = []
      coll_mediaRecorder = new MediaRecorder(coll_mediaStream)
      coll_mediaRecorder.ondataavailable = (e) => { if (e.data && e.data.size) coll_recordedBlobs.push(e.data) }
      coll_mediaRecorder.start()
      recordStatus.textContent = 'Recording...'
    })
    recStop.addEventListener('click', () => {
      if (coll_mediaRecorder) { coll_mediaRecorder.stop(); recordStatus.textContent = 'Stopped' }
    })
    uploadBtn.addEventListener('click', async () => {
      if (!coll_recordedBlobs || coll_recordedBlobs.length===0) { recordStatus.textContent='No recording'; return }
      const blob = new Blob(coll_recordedBlobs, {type:'audio/wav'})
      const fd = new FormData()
      fd.append('file', blob, 'sample.wav')
      fd.append('sample_type', document.getElementById('sampleType').value)
      fd.append('label', document.getElementById('sampleLabel').value)
      // Always use 'owner' for biometric training - backend will handle this
      fd.append('dataset', 'owner')
      recordStatus.textContent = 'Uploading owner voice sample...'
      try {
        const res = await fetch('/collect_sample', {method:'POST', body: fd})
        const j = await res.json()
        recordStatus.textContent = 'âœ“ Saved as owner voice: ' + (j.saved || JSON.stringify(j))
      } catch (e) { recordStatus.textContent = 'Upload failed: '+e }
    })
    
    // Test audio upload for debugging
    testAudioBtn.addEventListener('click', async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({audio:true})
        let chunks = []
        const recorder = new MediaRecorder(stream)
        recorder.ondataavailable = (e) => { if (e.data.size) chunks.push(e.data) }
        recorder.onstop = async () => {
          const blob = new Blob(chunks, {type: 'audio/wav'})
          console.log(`[TEST] Recording complete: ${blob.size} bytes, type=${blob.type}`)
          const fd = new FormData()
          fd.append('file', blob, 'test.wav')
          try {
            const res = await fetch('/test_audio_upload', {method: 'POST', body: fd})
            console.log(`[TEST] Response status: ${res.status}`)
            const result = await res.json()
            console.log(`[TEST] Result:`, result)
            alert('Audio test result:\n' + JSON.stringify(result, null, 2) + '\n\nSee browser console for details.')
          } catch (e) {
            console.error(`[TEST] Error:`, e)
            alert('Audio test failed: ' + e)
          }
          stream.getTracks().forEach(t => t.stop())
        }
        recorder.start()
        recordStatus.textContent = 'ðŸŽ¤ Recording 3 seconds for test...'
        setTimeout(() => { recorder.stop() }, 3000)
      } catch (e) {
        console.error('[TEST] Failed:', e)
        alert('Failed to start test: ' + e)
      }
    })

    // --- Live assistant: visualize and recognize speech ---
    const canvas = document.getElementById('wave')
    const ctx = canvas.getContext('2d')
    const recognized = document.getElementById('recognized')
    const assistantState = document.getElementById('assistantState')
    
    let audioCtx = null, analyser = null, dataArray = null, sourceNode = null
    let speechRec = null
    let armed = false
    let armTimer = null
    let errorCount = 0
    let lastErrorTime = 0
    const ARM_TIMEOUT = 5000  // 5 seconds
    const ERROR_COOLDOWN = 2000  // 2 seconds cooldown after errors
    const MAX_ERRORS = 5  // Max consecutive errors before showing message

    async function initializeAssistant() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({audio:true})
        audioCtx = new (window.AudioContext||window.webkitAudioContext)()
        analyser = audioCtx.createAnalyser(); analyser.fftSize = 2048
        sourceNode = audioCtx.createMediaStreamSource(stream)
        sourceNode.connect(analyser)
        dataArray = new Uint8Array(analyser.fftSize)
        drawWave()

        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition
        speechRec = new SpeechRecognition()
        speechRec.lang = 'en-US'
        // Use interim results for responsiveness
        speechRec.interimResults = true
        speechRec.continuous = true

        let recognitionActive = false

        function normText(s){ return s.toLowerCase().replace(/[^a-z0-9\s]/g,'').replace(/\s+/g,' ').trim() }

        // Expanded wakeword variants â€” add the forms you use (e.g. "jarvis", "hello jarvis")
        const wakePhrases = ['hey friday','hello jarvis','hey jarvis','jarvis','hello friday','hi friday','hi jarvis']
        const openPhrases = ['open door', 'open the door', 'open', 'door open', 'please open the door', 'please open door']
        const closePhrases = ['close door', 'close the door', 'door close', 'close', 'please close the door']

        // Try exact word-boundary match first, then fallback to substring inclusion for flexible matching
        function matchesList(text, list){
          for (const p of list){
            try {
              const pat = new RegExp('\\b' + p.replace(/[.*+?^${}()|[\\]\\]/g, '\\$&') + '\\b')
              if (pat.test(text)) return p
            } catch(e){ /* fallback if regex fails */ }
          }
          // fallback: substring check (looser)
          for (const p of list){ if (text.indexOf(p) !== -1) return p }
          return null
        }

        speechRec.onresult = async (ev) => {
          for (let ri = ev.resultIndex; ri < ev.results.length; ri++){
            const res = ev.results[ri]
            const text = res[0].transcript
            const normalized = normText(text)
            const isFinal = res.isFinal

            console.log('speech result', {index:ri, text, isFinal, armed})

            if (!armed){
              const w = matchesList(normalized, wakePhrases)
              if (w){
                // Wakeword detected - verify voice biometrically
                assistantState.textContent = 'ðŸ” Verifying voice...'
                recognized.textContent = '(verifying your voice)'
                const vw = await verifyVoiceWithServer(stream)
                console.log('[WAKEWORD VERIFY]', vw)
                if (vw && vw.verified) {
                  armed = true
                  recognized.textContent = '(waiting for command...)'
                  assistantState.textContent = 'ðŸŽ¤ ARMED â€” listening for command... (5 s)'
                  if (armTimer) clearTimeout(armTimer)
                  armTimer = setTimeout(() => { disarm() }, ARM_TIMEOUT)
                } else {
                  recognized.textContent = 'âŒ Voice not verified. Please try again.'
                  assistantState.textContent = 'ðŸŽ¤ Listening for wakeword...'
                }
                break
              }
            } else {
              if (isFinal){
                // Prioritize single-word main commands 'open' and 'close' for reliability
                const tokens = normalized.split(' ').filter(Boolean)
                let cmdName = null, cmdLabel = null
                
                if (tokens.includes('open') && !tokens.includes('close')){
                  cmdName = 'open_door'
                  cmdLabel = 'open'
                }
                else if (tokens.includes('close') && !tokens.includes('open')){
                  cmdName = 'close_door'
                  cmdLabel = 'close'
                }
                else {
                  // Fallback to phrase matching (e.g. 'open the door')
                  const o = matchesList(normalized, openPhrases)
                  if (o){
                    cmdName = 'open_door'
                    cmdLabel = o
                  }
                  else {
                    const c = matchesList(normalized, closePhrases)
                    if (c){
                      cmdName = 'close_door'
                      cmdLabel = c
                    }
                  }
                }
                
                if (cmdName) {
                  // Command detected - verify voice before executing
                  recognized.textContent = 'ðŸ” Verifying command voice...'
                  assistantState.textContent = 'ðŸ” Verifying voice...'
                  const vc = await verifyVoiceWithServer(stream)
                  console.log('[COMMAND VERIFY]', vc)
                  
                  if (vc && vc.verified) {
                    // Set light color based on command type
                    if (cmdName === 'open_door') {
                      setVerifiedLight('green')
                      console.log('[SET LIGHT] GREEN for open_door')
                    } else if (cmdName === 'close_door') {
                      setVerifiedLight('blue')
                      console.log('[SET LIGHT] BLUE for close_door')
                    }
                    recognized.textContent = `âœ“ COMMAND: ${cmdName} (${cmdLabel})`
                    try { sendCommandToServer(cmdName, cmdLabel) } catch(_){}
                    // Keep light on for 2 seconds before disarming
                    setTimeout(() => { disarm() }, 2000)
                  } else {
                    recognized.textContent = 'âŒ Voice not verified for command.'
                    disarm()
                  }
                  break
                }
                // No command detected in final result; keep waiting until timeout
              } else {
                // Interim: show what recognizer hears, but don't accept as final
                recognized.textContent = '(listening for command: ' + normalized + ')'
              }
            }
          }
        }

        function disarm(){
          armed = false
          assistantState.textContent = 'ðŸŽ¤ Listening for wakeword...'
          recognized.textContent = '(listening...)'
          if (armTimer){ clearTimeout(armTimer); armTimer = null }
          // turn off verified indicator when disarmed
          setVerifiedLight(false)
        }

        // Small visual indicator for successful biometric verification
        // color can be 'green', 'blue', false/null, or a hex value
        function setVerifiedLight(color){
          const el = document.getElementById('verifiedLight')
          if (!el) return
          if (!color) {
            el.style.background = '#777'
            el.style.boxShadow = 'none'
            return
          }
          if (color === 'green') {
            el.style.background = '#0f0'
            el.style.boxShadow = '0 0 6px rgba(0,255,0,0.8)'
          } else if (color === 'blue') {
            el.style.background = '#08f'
            el.style.boxShadow = '0 0 6px rgba(0,120,255,0.8)'
          } else {
            el.style.background = color
            el.style.boxShadow = '0 0 6px rgba(0,0,0,0.3)'
          }
        }
        speechRec.onstart = () => { 
          recognitionActive = true
          errorCount = 0  // Reset error count on successful start
          console.log('SpeechRec started') 
        }
        
        speechRec.onerror = (e) => {
          console.warn('SpeechRec error', e)
          const now = Date.now()
          
          // Handle different error types
          if (e.error === 'no-speech') {
            // No-speech is normal when user isn't talking - don't treat as error
            errorCount = Math.max(0, errorCount - 1)  // Decrease error count
            // Silently restart without showing error
            if (now - lastErrorTime > ERROR_COOLDOWN && !recognitionActive) {
              setTimeout(() => { 
                try { 
                  if (!recognitionActive) {
                    speechRec.start()
                    lastErrorTime = now
                  }
                } catch(_){} 
              }, 500)  // Slightly longer delay for no-speech
            }
          } else if (e.error === 'aborted') {
            // Aborted is intentional - don't restart
            recognitionActive = false
          } else if (e.error === 'network') {
            // Network error - show message and wait longer
            assistantState.textContent = 'âš  Network error - check connection'
            errorCount++
            if (now - lastErrorTime > ERROR_COOLDOWN * 2 && !recognitionActive) {
              setTimeout(() => { 
                try { 
                  if (!recognitionActive && errorCount < MAX_ERRORS) {
                    speechRec.start()
                    lastErrorTime = now
                  }
                } catch(_){} 
              }, 1000)
            }
          } else {
            // Other errors - show message
            if (e.error !== 'not-allowed') {  // Don't spam for permission errors
              assistantState.textContent = 'âš  Recognition error: ' + (e.error || 'unknown')
            }
            errorCount++
            
            // Only restart if not too many errors and cooldown passed
            if (now - lastErrorTime > ERROR_COOLDOWN && !recognitionActive && errorCount < MAX_ERRORS) {
              setTimeout(() => { 
                try { 
                  if (!recognitionActive) {
                    speechRec.start()
                    lastErrorTime = now
                  }
                } catch(_){} 
              }, 500)
            } else if (errorCount >= MAX_ERRORS) {
              assistantState.textContent = 'âš  Too many errors - please refresh the page'
              recognitionActive = false
            }
          }
        }

        speechRec.onend = () => {
          recognitionActive = false
          console.log('SpeechRec ended')
          
          // Only restart if not too many errors and cooldown passed
          const now = Date.now()
          if (now - lastErrorTime > ERROR_COOLDOWN && errorCount < MAX_ERRORS) {
            setTimeout(() => { 
              try { 
                if (!recognitionActive) {
                  speechRec.start()
                  lastErrorTime = now
                }
              } catch(_){} 
            }, 300)
          } else if (errorCount >= MAX_ERRORS) {
            assistantState.textContent = 'âš  Recognition stopped - too many errors. Please refresh.'
          }
        }

        try { speechRec.start() } catch(e){ console.warn('start failed', e) }
        assistantState.textContent = 'ðŸŽ¤ Listening for wakeword...'
      } catch (e) {
        assistantState.textContent = 'Failed to initialize: '+e.message
      }
    }

    // Send recognized command to backend so it appears in server terminal
    async function sendCommandToServer(command, raw){
      try {
        await fetch('/report_command', {
          method: 'POST',
          headers: {'Content-Type': 'application/json'},
          body: JSON.stringify({command: command, raw: raw, timestamp: Date.now()})
        })
      } catch (e) {
        console.warn('Failed to report command to server', e)
      }
    }

    // Verify voice against owner's biometric model
    let recordedChunks = []
    let mediaRecorder = null
    async function verifyVoiceWithServer(stream) {
      // Use a cloned MediaStream so the main microphone stream (used by SpeechRecognition)
      // is not disturbed. We stop cloned tracks after recording.
      // Returns full result object: { verified: bool, confidence: float, owner: string }
      return new Promise((resolve) => {
        recordedChunks = []
        const recStream = stream.clone()
        const recorder = new MediaRecorder(recStream)
        recorder.ondataavailable = (e) => {
          if (e.data && e.data.size) {
            recordedChunks.push(e.data)
            console.log(`[VERIFY] Chunk: ${e.data.size} bytes, type: ${e.data.type}`)
          }
        }
        recorder.onstop = async () => {
          const blob = new Blob(recordedChunks, {type: 'audio/wav'})
          console.log(`[VERIFY] Total blob: ${blob.size} bytes, type: ${blob.type}`)
          const formData = new FormData()
          formData.append('file', blob, 'verify.wav')
          try {
            const res = await fetch('/verify_voice', {method: 'POST', body: formData})
            console.log(`[VERIFY] Response status: ${res.status}`)
            if (!res.ok) {
              const errorText = await res.text()
              console.error(`[VERIFY] HTTP ${res.status}: ${errorText}`)
              recStream.getTracks().forEach(t => t.stop())
              resolve({verified: false})
              return
            }
            const result = await res.json()
            console.log('Voice verification result:', result)
            recStream.getTracks().forEach(t => t.stop())
            resolve(result)
          } catch (e) {
            console.warn('Voice verification failed:', e)
            recStream.getTracks().forEach(t => t.stop())
            resolve({verified: false})
          }
        }
        recorder.start()
        setTimeout(() => { recorder.stop() }, 2000)
      })
    }

    function drawWave() {
      if (!analyser) return
      requestAnimationFrame(drawWave)
      const w = canvas.width = canvas.clientWidth
      const h = canvas.height = canvas.clientHeight
      analyser.getByteTimeDomainData(dataArray)
      ctx.fillStyle = '#111'
      ctx.fillRect(0,0,w,h)
      ctx.lineWidth = 2
      ctx.strokeStyle = '#0f0'
      ctx.beginPath()
      const step = Math.max(1, Math.floor(dataArray.length / w))
      for (let i=0;i<w;i++){
        const v = dataArray[i*step]/128.0 -1.0
        const y = (v * 0.45 + 0.5) * h
        if (i===0) ctx.moveTo(i,y); else ctx.lineTo(i,y)
      }
      ctx.stroke()
    }

    // Auto-start assistant when page loads
    window.addEventListener('load', initializeAssistant)
  </script>
</body>
</html>